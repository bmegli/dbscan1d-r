<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{dbscan1d benchmark}
-->

dbscan1d benchmark
========================================================

This vignette benchmarks dbscan1d vs fpc::dbscan and dbscan::dbscan packages

Let's load the packages

```{r}
library(dbscan1d) #our library
library(microbenchmark) #benchmarking
```

Let's load alternative dbscan implementations

```{r}
library(fpc) # fpc::dbscan, no K-D trees O(n^2)
library(dbscan) # dbscan::dbscan, fast K-D trees backed with C++ implementation

```
Let's make a data preparation function.

Parameters:
- number of clusters
- average point distance in cluster
- number of points in cluster
- cluster separation ratio to cluster size

```{r}

test.data=function(clusters, points.distance, points, separation.ratio=1)
{
  cluster.radius=points.distance*points/2
  cluster.separation=cluster.radius*2*separation.ratio

  centers=seq(0, by=cluster.separation+2*cluster.radius, length.out=clusters)
  data=numeric()
  
  for(center in centers)
  {
    data=c(data, runif(points, center-cluster.radius, center+cluster.radius))
  }
  data=data[sample(length(data))]

  data
}    

data=test.data(4, 0.01, 100, 1)
head(data)

```

And 1D plot function with jitter as y:
```{r}

plot1d=function(data, ...)
{
  x=data
  y=jitter(rep(1, length(data)), amount=1)
  plot(x, y, type="p", axes=FALSE, ylab="",xlab="", ...)
  axis(side=1, at=c(min(x), max(x)), labels=c('',''),lwd.ticks = c(0, 0), pos=0, line=1)
}    

plot1d(data)

```

Let's test the clustering methods.


fpc::dbscan
dbscan::dbscan
dbscan1d

```{r}
eps=0.1
minPoints=4

data=test.data(clusters = 4, points.distance = 0.01, points = 100, separation.ratio = 1)

res.dbscan1d=dbscan1d(data, eps=eps, minPoints = minPoints)
res.dbscan=dbscan::dbscan(as.matrix(data), eps=eps, minPts=minPoints)
res.fpc=fpc::dbscan(data, eps=eps, MinPts=minPoints)

par(mfrow=c(3,1))

plot1d(data, col=res.dbscan1d, main="dbscan1d")
plot1d(data, col=res.dbscan$cluster, main="dbscan")
plot1d(data, col=res.fpc$cluster, main="fpc")
```

Let's make simple benchmark

```{r}

data=test.data(clusters = 4, points.distance = 0.01, points = 100, separation.ratio = 1)
data.matrix=as.matrix(data) #for dbscan::dbscan

eps=0.1
minPoints=4

bench=microbenchmark(dbscan1d(data, eps, minPoints), dbscan::dbscan(data.matrix, eps, minPoints), fpc::dbscan(data, eps, minPoints), times=100)

print(bench)
plot(bench)

```

Cleary fpc::dbscan R straightforward O(n^2) implementation is out of competition.

Let's focus on dbscan::dbscan vs dbscan1d

```{r}
data=test.data(clusters = 4, points.distance = 0.01, points = 1000, separation.ratio = 1)
data.matrix=as.matrix(data) #for dbscan::dbscan

eps=0.1
minPoints=4

bench=microbenchmark(dbscan1d(data, eps, minPoints),dbscan::dbscan(data.matrix, eps, minPoints), times=100)

print(bench)
plot(bench)

```

1. Mean/median are representative of the running time
2. dbscan1d is clearly a winner

Let's investigate dependence on input size:

```{r}
#benchmark parameters
N=10
inputs=1000*2^(0:(N-1))
times.dbscan1d=numeric(N)
times.dbscan=numeric(N)

#clustering parameters
points.distance=0.01
clusters=4
#we want to keep eps-neighborhood size ~ log(N) to let dbscan::dbscan work in (NlogN) time
epses=points.distance*log2(ceiling(clusters*inputs))/2
minPoints=10
#benchmark
for(i in 1:N)
{
  data=test.data(clusters = clusters, points.distance = points.distance, points = inputs[i], separation.ratio = 1)
  data.matrix=as.matrix(data) #for dbscan::dbscan
  bench=microbenchmark(dbscan1d(data, epses[i], minPoints), dbscan::dbscan(data.matrix, epses[i], minPoints), times=1, unit="ms")
  time.ms=summary(bench)$mean
  times.dbscan1d[i]=time.ms[1]
  times.dbscan[i]=time.ms[2]
}

ylim=c(min(c(log2(times.dbscan1d), log2(times.dbscan))), max(c(log2(times.dbscan1d), log2(times.dbscan) )))

plot(log2(inputs), log2(times.dbscan), type="p", ylim=ylim, ylab=expression(log[2](time)), xlab=expression(log[2](N)) , pch=3)
points(log2(inputs), log2(times.dbscan1d), type="p", pch=4)

# add nlogn plot in log/log space for comparison, the rintercept, chosen arbitrarily
x=c(log2(inputs[1]), log2(inputs[N]))
y=log2(c(inputs[1], inputs[N])*log2(c(inputs[1], inputs[N])))
lines(x,y-12.5)

legend("bottomright", legend=c("dbscan1d", "dbscan", "k*nlogn"), pch=c(3,4,NA_integer_), lty=c(0,0,1))
```



Let's investigate dependence on eps parameter

```{r}
#data
data=test.data(clusters = 4, points.distance = 0.01, points = 10000, separation.ratio = 1)
data.matrix=as.matrix(data) #for dbscan::dbscan

full.neighborhood=max(data)-min(data)

epses=seq(from=0.05, to=full.neighborhood/2, length.out = 10)
N=length(epses)
minPoints=4

times.dbscan1d=numeric(N)
times.dbscan=numeric(N)

#clustering parameters
#benchmark
for(i in 1:N)
{
  bench=microbenchmark(dbscan1d(data, epses[i], minPoints), dbscan::dbscan(data.matrix, epses[i], minPoints), times=10, unit="ms")
  time.ms=summary(bench)$mean
  times.dbscan1d[i]=time.ms[1]
  times.dbscan[i]=time.ms[2]
}

ylim=c(min(c(times.dbscan1d, times.dbscan)), max(c(times.dbscan1d, times.dbscan)))

plot(1:N, times.dbscan, type="p", col="red", ylim=ylim, ylab="ms")
points(1:N, times.dbscan1d, type="p", col="blue")

```
Let's investigate dependence on minPoints parameter

```{r}
#data
data=test.data(clusters = 4, points.distance = 0.01, points = 100, separation.ratio = 1)
data.matrix=as.matrix(data) #for dbscan::dbscan

full.neighborhood=max(data)-min(data)

eps=0.1
minPoints=4:20
N=length(minPoints)

times.dbscan1d=numeric(N)
times.dbscan=numeric(N)

#clustering parameters
#benchmark
for(i in 1:N)
{
  bench=microbenchmark(dbscan1d(data, eps, minPoints[i]), dbscan::dbscan(data.matrix, eps, minPoints[i]), times=10, unit="ms")
  time.ms=summary(bench)$mean
  times.dbscan1d[i]=time.ms[1]
  times.dbscan[i]=time.ms[2]
}

ylim=c(min(c(times.dbscan1d, times.dbscan)), max(c(times.dbscan1d, times.dbscan)))

plot(1:N, times.dbscan, type="p", col="red", ylim=ylim, ylab="ms")
points(1:N, times.dbscan1d, type="p", col="blue")

```
